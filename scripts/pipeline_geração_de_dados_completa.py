# -*- coding: utf-8 -*-
"""Pipeline Gera√ß√£o de Dados Completa

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dy4kZksX78NiHr2-5rcnzHeiGw4H3zV_
"""

from google.colab import drive
drive.mount('/content/drive')

# 1_geracao/base1_participacao.py

import pandas as pd
import numpy as np
import os
import random

print("Iniciando a gera√ß√£o da Base 1: Participantes")

# --- Par√¢metros e Pesos para Realismo ---
np.random.seed(42)
random.seed(42)

NUM_PARTICIPANTES = 10000
DIAS_FESTIVAL = ["Pop", "Rock", "Hip-Hop/Rap", "M√∫sica Brasileira", "Eletr√¥nico"]

# Distribui√ß√£o de localiza√ß√£o mais realista
LOCALIZACAO_PESOS = {
    "Rio de Janeiro": 0.45, "S√£o Paulo": 0.25, "Belo Horizonte": 0.10,
    "Vit√≥ria": 0.05, "Outros": 0.15
}
LOC_DEFEITOS = {
    "Rio de Janeiro": " rj ", "S√£o Paulo": "sao paulo", "Belo Horizonte": "BH"
}

FREQUENCIA_OPCOES = ["Primeira vez", "Ocasional", "Veterano(a)"]

# --- Gera√ß√£o Coluna por Coluna com Causalidade ---

# 1. IDs e Demografia B√°sica
ids = np.arange(1, NUM_PARTICIPANTES + 1)
idades = np.clip(np.random.normal(loc=28, scale=8, size=NUM_PARTICIPANTES), 18, 65)
generos = np.random.choice(["Masculino", "Feminino", "Outro"], p=[0.48, 0.48, 0.04], size=NUM_PARTICIPANTES)

# 2. Localiza√ß√£o (com pesos e defeitos)
localizacoes_limpas = random.choices(
    list(LOCALIZACAO_PESOS.keys()), weights=list(LOCALIZACAO_PESOS.values()), k=NUM_PARTICIPANTES
)
localizacoes_com_defeitos = []
for loc in localizacoes_limpas:
    if random.random() < 0.15: # Introduz defeitos em 15% dos dados
        localizacoes_com_defeitos.append(LOC_DEFEITOS.get(loc, loc))
    else:
        localizacoes_com_defeitos.append(loc)

# 3. Dia do Festival (Causalidade com Idade)
dias_festival = []
for idade in idades:
    if idade < 25:
        dia = np.random.choice(DIAS_FESTIVAL, p=[0.35, 0.15, 0.35, 0.05, 0.1])
    elif idade >= 40:
        dia = np.random.choice(DIAS_FESTIVAL, p=[0.05, 0.40, 0.05, 0.40, 0.1])
    else:
        dia = np.random.choice(DIAS_FESTIVAL)
    dias_festival.append(dia)

# 4. Valor Gasto no Evento (Causalidade com Dia do Festival)
# A l√≥gica simula que o gasto m√©dio varia conforme o dia do festival.
valores_gasto = []
for dia in dias_festival:
    # A base de gasto usa log-normal para realismo (muitos gastam moderadamente, poucos gastam muito)
    base_gasto = np.random.lognormal(mean=5.5, sigma=0.5) # Mediana em torno de R$244

    # Aplica multiplicadores baseados no dia do festival
    if dia in ["Pop", "Rock"]:
        gasto = base_gasto * 1.2
    elif dia == "Hip-Hop/Rap":
        gasto = base_gasto * 1.1
    else:
        gasto = base_gasto * 0.9

    valores_gasto.append(gasto)

valores_gasto = np.array(valores_gasto)
# Introduz valores nulos para simular participantes que n√£o registraram gastos
valores_gasto[np.random.choice(valores_gasto.size, int(NUM_PARTICIPANTES * 0.10), replace=False)] = np.nan

# 5. Frequ√™ncia (N√£o Causal)
frequencia_festival = np.random.choice(FREQUENCIA_OPCOES, p=[0.4, 0.35, 0.25], size=NUM_PARTICIPANTES)

# 6. Nota do Lineup (Causal com Dia/G√™nero)
notas_lineup = []
for dia in dias_festival:
    if dia in ["Pop", "Rock"]:
        nota = np.random.normal(loc=8.5, scale=1.5)
    elif dia == "M√∫sica Brasileira":
        nota = np.random.normal(loc=7.5, scale=1.8)
    else:
        nota = np.random.normal(loc=7.0, scale=2.0)
    notas_lineup.append(round(np.clip(nota, 1, 10), 1))

# 7. Satisfa√ß√£o Geral com o Festival (Causal com Frequ√™ncia)
satisfacao_festival = []
for freq in frequencia_festival:
    if freq == "Primeira vez":
        satisfacao = np.random.normal(loc=9.0, scale=1.0)
    elif freq == "Ocasional":
        satisfacao = np.random.normal(loc=8.0, scale=1.5)
    else:
        satisfacao = np.random.normal(loc=7.5, scale=2.0)
    satisfacao_festival.append(round(np.clip(satisfacao, 1, 10), 1))

# --- Montagem e Finaliza√ß√£o do DataFrame ---
df_participantes = pd.DataFrame({
    'id_participante': ids,
    'idade': idades.astype(int),
    'genero': generos,
    'localizacao': localizacoes_com_defeitos,
    'valor_gasto': np.round(valores_gasto, 2),
    'frequencia_festival': frequencia_festival,
    'dia_festival': dias_festival,
    'nota_lineup': notas_lineup,
    'satisfacao_festival': satisfacao_festival
})

# Introduzindo defeito final: tipo de dado inconsistente em 'idade'
indices_defeito = df_participantes.sample(frac=0.1, random_state=42).index
df_participantes.loc[indices_defeito, 'idade'] = df_participantes.loc[indices_defeito, 'idade'].astype(str)

# --- Salvando o arquivo ---
output_dir = "data/raw"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, 'base1_participantes.csv')
df_participantes.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"Base 1 gerada com sucesso! {len(df_participantes)} linhas.")
print(f"Arquivo salvo em: {output_path}")
print("\nAmostra dos dados gerados:")
print(df_participantes.head())

# 1_geracao/base2_marcas.py

import pandas as pd
import numpy as np
import os
import random

print("Iniciando a gera√ß√£o da Base 2 (vers√£o com Segmentos): Intera√ß√µes de Mercado")

# --- Configura√ß√µes ---
np.random.seed(42)
random.seed(42)

# --- Carregar a Base 1 para usar seus dados na gera√ß√£o ---
path_b1 = "data/raw/base1_participantes.csv"
if not os.path.exists(path_b1):
    print(f"Erro: Arquivo '{path_b1}' n√£o encontrado. Execute o script base1_participacao.py primeiro.")
    exit()

df_participantes = pd.read_csv(path_b1)

# --- Par√¢metros dos Segmentos e Intera√ß√µes (SEM NOMES DE MARCAS) ---
SEGMENTOS_DISPONIVEIS = [
    "Servi√ßos Financeiros", "Bebida Alco√≥lica", "Postos de Combust√≠vel",
    "Alimentos", "Seguros", "Telecomunica√ß√µes", "Moda", "Automobilismo",
    "Bebida N√£o Alco√≥lica", "Beleza", "Chocolates"
]
TIPO_INTERACAO_PESOS = {
    "Compra de Produto": 0.5,
    "Visita a Estande": 0.3,
    "Participou de Ativa√ß√£o": 0.2
}

# --- Gera√ß√£o das Intera√ß√µes ---
interacoes_lista = []
id_interacao_counter = 1

print("Gerando intera√ß√µes com segmentos para cada participante...")
for _, participante in df_participantes.iterrows():
    # Cada participante interage com 1 a 4 segmentos
    num_interacoes = random.randint(1, 4)
    segmentos_interagidos = random.sample(SEGMENTOS_DISPONIVEIS, k=num_interacoes)

    for segmento in segmentos_interagidos:
        # Gerando as vari√°veis independentes primeiro
        tempo_de_espera = round(np.random.lognormal(mean=2.5, sigma=0.8)) # Em minutos
        avaliacao_experiencia = random.randint(1, 5)

        if random.random() < 0.4: # 40% de chance de ter brinde
            avaliacao_brinde = random.randint(1, 5)
        else:
            avaliacao_brinde = np.nan # Defeito: NaN para intera√ß√µes sem brinde

        # Rela√ß√£o Causal #1 (Forte): satisfacao_marca depende de experiencia, tempo e brinde
        tempo_espera_normalizado = min(tempo_de_espera / 30, 1) # Penalidade m√°xima ap√≥s 30 min
        nota_base = (avaliacao_experiencia * 0.7) - (tempo_espera_normalizado * 0.5)
        if not np.isnan(avaliacao_brinde):
             nota_base += (avaliacao_brinde * 0.2)
        satisfacao_segmento = round(np.clip(nota_base + np.random.normal(0, 0.3), 1, 5), 1)

        # Rela√ß√£o Causal #5 (Esperada): recomendaria_segmento depende da satisfa√ß√£o
        prob_recomendar = (satisfacao_segmento - 1) / 4
        recomendaria_segmento = random.random() < prob_recomendar

        interacoes_lista.append({
            'id_interacao': id_interacao_counter,
            'id_participante': participante['id_participante'],
            'segmento_interagido': segmento,
            'tipo_interacao': random.choices(list(TIPO_INTERACAO_PESOS.keys()), weights=list(TIPO_INTERACAO_PESOS.values()), k=1)[0],
            'tempo_de_espera': tempo_de_espera,
            'avaliacao_experiencia_ativacao': avaliacao_experiencia,
            'avaliacao_brinde': avaliacao_brinde,
            'satisfacao_segmento': satisfacao_segmento,
            'recomendaria_segmento': recomendaria_segmento
        })
        id_interacao_counter += 1

df_interacoes = pd.DataFrame(interacoes_lista)

# --- Introdu√ß√£o de Defeitos Adicionais para Pr√©-processamento ---
# 1. Defeito no tempo_de_espera: Alguns valores como string "X min"
indices_defeito_tempo = df_interacoes.sample(frac=0.1, random_state=1).index
df_interacoes.loc[indices_defeito_tempo, 'tempo_de_espera'] = df_interacoes.loc[indices_defeito_tempo, 'tempo_de_espera'].astype(str) + ' min'

# 2. Defeito em satisfacao_segmento: Outliers (valores fora da escala 1-5)
indices_defeito_satisfacao = df_interacoes.sample(frac=0.03, random_state=2).index
df_interacoes.loc[indices_defeito_satisfacao, 'satisfacao_segmento'] = random.choices([0, -1, 6, 7.5], k=len(indices_defeito_satisfacao))

# --- Passo de Enriquecimento: Atualizar a Base 1 ---
# Rela√ß√£o Causal #7: O segmento mais lembrado √© aquele com maior satisfa√ß√£o
print("Enriquecendo a Base 1 com o segmento mais lembrado...")
idx_max_satisfacao = df_interacoes.groupby('id_participante')['satisfacao_segmento'].idxmax()
df_melhor_experiencia = df_interacoes.loc[idx_max_satisfacao][['id_participante', 'segmento_interagido']]

mapeamento_segmento = df_melhor_experiencia.set_index('id_participante')['segmento_interagido']
df_participantes['segmento_mais_lembrado'] = df_participantes['id_participante'].map(mapeamento_segmento)

# --- Salvando os Arquivos ---
output_dir = "data/raw"
os.makedirs(output_dir, exist_ok=True)

# Salva a Base 2
path_b2 = os.path.join(output_dir, 'base2_marcas.csv')
df_interacoes.to_csv(path_b2, index=False, encoding='utf-8-sig')
print(f"\nBase 2 gerada com sucesso! {len(df_interacoes)} linhas.")
print(f"Arquivo salvo em: {path_b2}")

# Salva novamente a Base 1, agora enriquecida
df_participantes.to_csv(path_b1, index=False, encoding='utf-8-sig')
print(f"\nBase 1 foi atualizada/enriquecida com a coluna 'segmento_mais_lembrado'.")
print(f"Arquivo salvo em: {path_b1}")

print("\nüîç Verifica√ß√£o da Base 2:")
print("---------------------------------------")
print("Amostra da base de intera√ß√µes com segmentos:")
print(df_interacoes.sample(5, random_state=42))
print("\nVerifica√ß√£o do enriquecimento na Base 1:")
print(df_participantes[['id_participante', 'segmento_mais_lembrado']].dropna().head())

# 1_geracao/base3_redes.py

import pandas as pd
import numpy as np
import os
import random
from datetime import datetime, timedelta

print("Iniciando a gera√ß√£o da Base 3 (vers√£o aprimorada com Alcance, Impress√µes e Salvamentos)")

# --- Configura√ß√µes ---
np.random.seed(42)
random.seed(42)

DIAS_FESTIVAL = ["Pop", "Rock", "Hip-Hop/Rap", "M√∫sica Brasileira", "Eletr√¥nico"]
REDES_SOCIAIS = ["Instagram", "Twitter", "TikTok"]
DATA_INICIO_FESTIVAL = datetime(2024, 9, 13)
POSTS_POR_DIA_POR_REDE = 10

# --- Gera√ß√£o dos Posts ---
posts_lista = []
id_post_counter = 1

print("Gerando posts da campanha da marca para cada dia do festival...")
for i, dia in enumerate(DIAS_FESTIVAL):
    data_do_dia = DATA_INICIO_FESTIVAL + timedelta(days=i)

    for rede in REDES_SOCIAIS:
        for _ in range(POSTS_POR_DIA_POR_REDE):

            # --- L√≥gica de Gera√ß√£o Revisada (Funil de Marketing Digital) ---

            # 1. Estrat√©gia de Conte√∫do (como antes)
            if rede == "TikTok": tipo_conteudo = "V√≠deo Curto"
            elif rede == "Instagram": tipo_conteudo = random.choice(["Reels", "V√≠deo Curto"]) if dia in ["Pop", "Eletr√¥nico"] else random.choice(["Foto", "Carrossel"])
            else: tipo_conteudo = "Texto com Imagem"

            # 2. Gera√ß√£o de Alcance (Reach) - Pessoas √önicas
            # Simula que TikTok tem maior alcance org√¢nico, seguido por Instagram.
            if rede == "TikTok": base_alcance = np.random.randint(80000, 300000)
            elif rede == "Instagram": base_alcance = np.random.randint(50000, 200000)
            else: base_alcance = np.random.randint(10000, 50000)

            # Formatos de v√≠deo t√™m maior potencial de alcance
            if tipo_conteudo in ["Reels", "V√≠deo Curto"]: base_alcance *= 1.4

            # Multiplicador de "hype" do festival
            multiplicador_dia = 1.3 - (i * 0.1)
            alcance = int(base_alcance * multiplicador_dia)

            # 3. Gera√ß√£o de Impress√µes (Impressions) a partir do Alcance
            # Impress√µes = Alcance * Frequ√™ncia (quantas vezes em m√©dia cada pessoa viu)
            frequencia = np.random.uniform(1.1, 1.8) # Cada pessoa viu o post entre 1.1 e 1.8 vezes em m√©dia
            impressoes = int(alcance * frequencia)

            # 4. Gera√ß√£o de Curtidas a partir das Impress√µes
            # Taxa de engajamento (curtidas/impress√µes) varia
            taxa_de_curtidas = np.random.uniform(0.03, 0.12) # Entre 3% e 12%
            curtidas = int(impressoes * taxa_de_curtidas)

            # 5. Gera√ß√£o de outras intera√ß√µes a partir das Curtidas
            comentarios = int(curtidas * np.random.uniform(0.01, 0.04))
            compartilhamentos = int(curtidas * np.random.uniform(0.02, 0.05))
            # Salvamentos s√£o uma a√ß√£o de alta inten√ß√£o, geralmente menos frequentes
            salvamentos = int(curtidas * np.random.uniform(0.005, 0.03))


            # --- Introdu√ß√£o de Defeitos (como antes) ---
            if random.random() < 0.3: data_postagem = data_do_dia.strftime('%d/%m/%Y')
            else: data_postagem = data_do_dia.strftime('%Y-%m-%d')

            posts_lista.append({
                'id_post': id_post_counter,
                'data_postagem': data_postagem,
                'rede_social': rede,
                'dia_festival': dia,
                'tipo_conteudo': tipo_conteudo,
                'alcance': alcance,
                'impressoes': impressoes,
                'curtidas': curtidas,
                'comentarios': comentarios,
                'compartilhamentos': compartilhamentos,
                'salvamentos': salvamentos
            })
            id_post_counter += 1

df_redes = pd.DataFrame(posts_lista)

# Defeito no formato das m√©tricas (formato "k")
indices_defeito_k = df_redes[df_redes['curtidas'] > 1000].sample(frac=0.2, random_state=1).index
for idx in indices_defeito_k:
    valor = df_redes.loc[idx, 'curtidas']
    df_redes.loc[idx, 'curtidas'] = f"{valor/1000:.1f}k"
    valor_comentario = df_redes.loc[idx, 'comentarios']
    if valor_comentario > 1000:
       df_redes.loc[idx, 'comentarios'] = f"{valor_comentario/1000:.1f}k"

# --- Salvando o arquivo ---
output_dir = "data/raw"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, 'base3_redes.csv')
df_redes.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"\nBase 3 (vers√£o final) gerada com sucesso! {len(df_redes)} linhas.")
print(f"Arquivo salvo em: {output_path}")

print("\nAmostra dos dados gerados com as novas m√©tricas:")
display(df_redes.head())

# 1_geracao/base4_marca_aberta.py

import pandas as pd
import numpy as np
import os
import random

try:
    from faker import Faker
    fake = Faker('pt_BR')
except ImportError:
    print("Biblioteca Faker n√£o encontrada. Usando fallback para nomes de usu√°rio.")
    fake = None

print("Iniciando a gera√ß√£o da Base 4 (vers√£o 'deep dive'): Men√ß√µes Abertas dos Usu√°rios")

# --- Configura√ß√µes ---
np.random.seed(42)
random.seed(42)

# --- Carregar as bases anteriores ---
path_b1 = "data/raw/base1_participantes.csv"
path_b2 = "data/raw/base2_marcas.csv"

if not os.path.exists(path_b1) or not os.path.exists(path_b2):
    print("Erro: Arquivos das bases 1 ou 2 n√£o encontrados. Execute os scripts anteriores primeiro.")
    exit()

df_participantes = pd.read_csv(path_b1)
df_interacoes = pd.read_csv(path_b2, low_memory=False)

# --- Limpeza preliminar da Base 2 ---
def limpar_tempo(valor):
    if isinstance(valor, str):
        return int(valor.replace(' min', ''))
    return int(valor)
df_interacoes['tempo_de_espera_limpo'] = df_interacoes['tempo_de_espera'].apply(limpar_tempo)

# --- L√≥gica de "Deep Dive" em um Segmento Espec√≠fico ---
SEGMENTO_ALVO = "Bebida Alco√≥lica"
print(f"Filtrando intera√ß√µes para gerar men√ß√µes apenas para o segmento: '{SEGMENTO_ALVO}'")

# Para adicionar o 'dia_festival', precisamos primeiro juntar a Base 1 e 2
# Apenas as colunas necess√°rias para evitar sobrecarga de mem√≥ria
df_interacoes_enriquecido = pd.merge(
    df_interacoes,
    df_participantes[['id_participante', 'dia_festival']],
    on='id_participante',
    how='left'
)

# Agora, filtramos o dataframe j√° enriquecido
df_interacoes_filtrado = df_interacoes_enriquecido[df_interacoes_enriquecido['segmento_interagido'] == SEGMENTO_ALVO].copy()

if df_interacoes_filtrado.empty:
    print(f"Nenhuma intera√ß√£o encontrada para o segmento '{SEGMENTO_ALVO}'. A Base 4 n√£o ser√° gerada.")
    exit()

# --- Par√¢metros ---
fracao_amostra = 0.5
NUM_MENCOES = int(len(df_interacoes_filtrado) * fracao_amostra)
REDES_SOCIAIS = ['Twitter', 'Instagram']
TEMPLATES = {
    'elogio': [
        "A experi√™ncia no estande de {segmento} foi SENSACIONAL! Valeu cada segundo. #Festival",
        "Que organiza√ß√£o incr√≠vel no espa√ßo de {segmento}. Atendimento nota 10/10. ‚ú®",
        "Simplesmente amei o brinde que ganhei de {segmento}! J√° quero o pr√≥ximo festival."
    ],
    'reclamacao': [
        "Absurdo o tempo de espera na fila de {segmento}: {tempo_espera} minutos pra nada. üò° #Fail",
        "Totalmente decepcionado com a ativa√ß√£o de {segmento}. Esperava muito mais.",
        "O pessoal de {segmento} parecia perdido, atendimento p√©ssimo."
    ],
    'neutro': [
        "Passei pelo estande de {segmento} hoje no festival.",
        "Vi que {segmento} √© um dos patrocinadores este ano.",
        "Algu√©m sabe o que t√° rolando na ativa√ß√£o de {segmento}?"
    ]
}

# --- Gera√ß√£o dos Posts ---
mencoes_lista = []
id_mencao_counter = 1
print(f"Gerando {NUM_MENCOES} men√ß√µes de usu√°rios com base em suas intera√ß√µes com '{SEGMENTO_ALVO}'...")

amostra_interacoes = df_interacoes_filtrado.sample(n=NUM_MENCOES, random_state=42)

for _, interacao in amostra_interacoes.iterrows():
    satisfacao = interacao['satisfacao_segmento']
    tempo_espera = interacao['tempo_de_espera_limpo']
    dia_do_festival = interacao['dia_festival'] # Capturando o dia do festival

    # Definir o tom do post com base na satisfa√ß√£o
    if satisfacao >= 4.0: tema_conteudo = 'elogio'
    elif satisfacao <= 2.5: tema_conteudo = 'reclamacao'
    else: tema_conteudo = 'neutro'

    conteudo = random.choice(TEMPLATES[tema_conteudo]).format(
        segmento=SEGMENTO_ALVO, tempo_espera=tempo_espera
    )

    # Adicionar ru√≠do lingu√≠stico
    if random.random() < 0.3: conteudo = conteudo.replace("que", "q").replace("nao", "n")
    if random.random() < 0.6: conteudo += " " + random.choice(['kkkkk', 'üòÇ', 'ü§°', 'top', 'aff', 'ü§î'])

    # Gerar nome de usu√°rio
    if fake: nome_usuario = fake.first_name().lower() + str(random.randint(10, 99))
    else: nome_usuario = f"user{interacao['id_participante']}"

    mencoes_lista.append({
        'id_mencao': id_mencao_counter,
        'id_participante': interacao['id_participante'],
        'nome_usuario': nome_usuario,
        'rede_social': random.choice(REDES_SOCIAIS),
        'segmento_mencionado': SEGMENTO_ALVO,      # <-- COLUNA PREENCHIDA DIRETAMENTE
        'dia_festival': dia_do_festival,           # <-- NOVA COLUNA ADICIONADA
        'tema_conteudo': tema_conteudo,
        'conteudo_post': conteudo
    })
    id_mencao_counter += 1

df_mencoes = pd.DataFrame(mencoes_lista)

# --- Salvando o arquivo ---
output_dir = "data/raw"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, 'base4_marca_aberta.csv')
df_mencoes.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"\nBase 4 gerada com sucesso! {len(df_mencoes)} linhas.")
print(f"Arquivo salvo em: {output_path}")

print("\nAmostra dos dados gerados:")
display(df_mencoes.head())

# 2_unificacao.py

import pandas as pd
import os

print("--- Iniciando Etapa 2: Unifica√ß√£o dos Dados ---")

# --- 1. Defini√ß√£o dos Caminhos ---
INPUT_DIR = "data/raw"
OUTPUT_DIR = "data/interim" # Usaremos um diret√≥rio intermedi√°rio para os dados processados

# Criar o diret√≥rio de sa√≠da se ele n√£o existir
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Caminhos dos arquivos de entrada
path_b1 = os.path.join(INPUT_DIR, 'base1_participantes.csv')
path_b2 = os.path.join(INPUT_DIR, 'base2_marcas.csv')
path_b3 = os.path.join(INPUT_DIR, 'base3_redes.csv')
path_b4 = os.path.join(INPUT_DIR, 'base4_marca_aberta.csv')

# Caminho do arquivo de sa√≠da
output_path = os.path.join(OUTPUT_DIR, 'dados_unificados_para_processamento.csv')


# --- 2. Carregamento dos Dados ---
try:
    print(f"Carregando dados de '{path_b1}'...")
    df_participantes = pd.read_csv(path_b1)

    print(f"Carregando dados de '{path_b2}'...")
    df_interacoes = pd.read_csv(path_b2, low_memory=False)

    print(f"Carregando dados de '{path_b3}'...")
    df_redes_sociais = pd.read_csv(path_b3)

    print(f"Carregando dados de '{path_b4}'...")
    df_mencoes = pd.read_csv(path_b4)

    print("\nTodos os arquivos foram carregados com sucesso.")

except FileNotFoundError as e:
    print(f"\nERRO: Arquivo n√£o encontrado. {e}")
    print("Por favor, certifique-se de que todos os scripts da etapa '1_geracao' foram executados.")
    exit()

# --- 3. Unifica√ß√£o Principal (Jun√ß√£o da Base 1 e Base 2) ---
# O "gr√£o" da nossa an√°lise principal ser√° a INTERA√á√ÉO.
# Portanto, vamos enriquecer a tabela de intera√ß√µes com os dados dos participantes.
# Usamos um left join para garantir que todas as intera√ß√µes sejam mantidas.

print("\nUnificando a base de intera√ß√µes com a base de participantes...")
df_principal = pd.merge(
    df_interacoes,
    df_participantes,
    on='id_participante',
    how='left'
)

print(f"A tabela principal unificada tem {len(df_principal)} linhas e {len(df_principal.columns)} colunas.")

# --- 4. Armazenamento ---
# Neste ponto, salvamos a tabela principal unificada.
# As outras tabelas (df_redes_sociais e df_mencoes) ser√£o usadas na pr√≥xima etapa,
# mas n√£o precisam ser juntadas agora para evitar duplica√ß√£o massiva de dados.

print(f"\nSalvando a tabela principal unificada em '{output_path}'...")
df_principal.to_csv(output_path, index=False, encoding='utf-8-sig')

print("\n--- Etapa 2: Unifica√ß√£o conclu√≠da com sucesso! ---")
print(f"O arquivo principal para a pr√≥xima etapa est√° pronto.")
print("\nAmostra da tabela unificada:")
print(df_principal.head())

# 3_preprocessamento.py

import pandas as pd
import numpy as np
import os

print("--- Iniciando Etapa 3: Pr√©-processamento e Limpeza dos Dados ---")

# --- 1. Defini√ß√£o dos Caminhos ---
INPUT_DIR = "data/interim"
RAW_DIR = "data/raw"
OUTPUT_DIR = "data/processed"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# Caminhos dos arquivos de entrada
path_principal = os.path.join(INPUT_DIR, 'dados_unificados_para_processamento.csv')
path_redes = os.path.join(RAW_DIR, 'base3_redes.csv')
path_mencoes = os.path.join(RAW_DIR, 'base4_marca_aberta.csv')

# --- 2. Carregamento dos Dados ---
try:
    print("Carregando dados unificados...")
    df_principal = pd.read_csv(path_principal, low_memory=False)

    print("Carregando dados de redes sociais...")
    df_redes = pd.read_csv(path_redes)

    print("Carregando dados de men√ß√µes...")
    df_mencoes = pd.read_csv(path_mencoes)

    print("\nArquivos carregados com sucesso.")
except FileNotFoundError as e:
    print(f"\nERRO: Arquivo n√£o encontrado. {e}")
    print("Certifique-se de que as etapas 1 e 2 foram executadas.")
    exit()


# --- 3. Fun√ß√µes de Limpeza (Helpers) ---
def padronizar_localizacao(loc):
    """Padroniza a coluna de localiza√ß√£o, corrigindo abrevia√ß√µes e capitaliza√ß√£o."""
    if pd.isna(loc): return None
    loc = str(loc).strip().lower()
    mapeamento = {'rj': 'Rio de Janeiro', 'sao paulo': 'S√£o Paulo', 'bh': 'Belo Horizonte'}
    return mapeamento.get(loc, loc.title())

def limpar_metricas_k(valor):
    """Converte m√©tricas de texto com 'k' (e.g., '15.2k') em valores num√©ricos."""
    if isinstance(valor, str):
        valor = valor.lower()
        if 'k' in valor:
            return float(valor.replace('k', '')) * 1000
    return pd.to_numeric(valor, errors='coerce')

# --- 4. Processamento da Tabela Principal ---
print("\nIniciando limpeza da tabela principal...")
df_principal_limpo = df_principal.copy()

# Tratamento da coluna 'idade'
# Converte para num√©rico, tratando erros que foram inseridos propositalmente.
if 'idade' in df_principal_limpo.columns:
    df_principal_limpo['idade'] = pd.to_numeric(df_principal_limpo['idade'], errors='coerce')
    # Imputa a mediana para quaisquer valores que n√£o puderam ser convertidos
    mediana_idade = df_principal_limpo['idade'].median()
    df_principal_limpo['idade'].fillna(mediana_idade, inplace=True)
    df_principal_limpo['idade'] = df_principal_limpo['idade'].astype(int)

# Tratamento da coluna 'localizacao'
# Aplica a fun√ß√£o de padroniza√ß√£o para unificar os nomes das cidades.
if 'localizacao' in df_principal_limpo.columns:
    df_principal_limpo['localizacao'] = df_principal_limpo['localizacao'].apply(padronizar_localizacao)

# Tratamento da coluna 'valor_gasto' (antiga renda_mensal)
# Imputa a mediana para preencher valores ausentes (NaN).
if 'valor_gasto' in df_principal_limpo.columns:
    mediana_gasto = df_principal_limpo['valor_gasto'].median()
    df_principal_limpo['valor_gasto'].fillna(mediana_gasto, inplace=True)
else:
    print("Aviso: Coluna 'valor_gasto' n√£o encontrada na tabela principal.")

# Tratamento da coluna 'tempo_de_espera'
# Remove o sufixo "min" e converte a coluna para o tipo num√©rico.
if 'tempo_de_espera' in df_principal_limpo.columns:
    df_principal_limpo['tempo_de_espera'] = df_principal_limpo['tempo_de_espera'].astype(str).str.replace('min', '', regex=False).str.strip()
    df_principal_limpo['tempo_de_espera'] = pd.to_numeric(df_principal_limpo['tempo_de_espera'], errors='coerce')

# Tratamento da coluna 'avaliacao_brinde'
# Imputa 0 nos valores nulos, representando explicitamente a aus√™ncia de brinde.
if 'avaliacao_brinde' in df_principal_limpo.columns:
    df_principal_limpo['avaliacao_brinde'].fillna(0, inplace=True)

# Tratamento da coluna 'satisfacao_segmento'
# Aplica a t√©cnica de "clipping" para corrigir outliers e garantir que os valores fiquem na escala de 1 a 5.
if 'satisfacao_segmento' in df_principal_limpo.columns:
    df_principal_limpo['satisfacao_segmento'] = df_principal_limpo['satisfacao_segmento'].clip(lower=1, upper=5)

print("Limpeza da tabela principal conclu√≠da.")

# --- 5. Processamento da Tabela de Redes Sociais ---
print("\nIniciando limpeza da tabela de redes sociais...")
df_redes_limpo = df_redes.copy()

# Tratamento da coluna 'data_postagem'
# Converte os formatos mistos (ISO e brasileiro) para um tipo datetime padronizado.
df_redes_limpo['data_postagem'] = pd.to_datetime(df_redes_limpo['data_postagem'], errors='coerce', dayfirst=False, format=None)

# Tratamento das colunas de m√©tricas
# Aplica a fun√ß√£o de limpeza para converter valores como "37.8k" em 37800.
df_redes_limpo['curtidas'] = df_redes_limpo['curtidas'].apply(limpar_metricas_k)
df_redes_limpo['comentarios'] = df_redes_limpo['comentarios'].apply(limpar_metricas_k)
df_redes_limpo['compartilhamentos'] = pd.to_numeric(df_redes_limpo['compartilhamentos'], errors='coerce')

print("Limpeza da tabela de redes sociais conclu√≠da.")

# --- 6. Salvando os Dados Processados ---
print("\nSalvando arquivos processados...")

path_principal_limpo = os.path.join(OUTPUT_DIR, 'dados_principais_limpos.csv')
df_principal_limpo.to_csv(path_principal_limpo, index=False, encoding='utf-8-sig')

path_redes_limpo = os.path.join(OUTPUT_DIR, 'dados_redes_limpos.csv')
df_redes_limpo.to_csv(path_redes_limpo, index=False, encoding='utf-8-sig')

# A base de men√ß√µes √© salva sem grandes altera√ß√µes estruturais, pronta para o NLP.
path_mencoes_processado = os.path.join(OUTPUT_DIR, 'dados_mencoes_para_nlp.csv')
df_mencoes.to_csv(path_mencoes_processado, index=False, encoding='utf-8-sig')

print(f"Arquivos salvos em '{OUTPUT_DIR}'")

# --- 7. Verifica√ß√£o Final ---
print("\n--- Verifica√ß√£o Final da Tabela Principal Limpa ---")
df_principal_limpo.info()

print("\nAmostra da tabela principal ap√≥s a limpeza:")
print(df_principal_limpo.head())

!pip install pysentimiento

# 4_enriquecimento.py

import pandas as pd
import numpy as np
import os
import re
# Importando a nova biblioteca para an√°lise de sentimento
from pysentimiento import create_analyzer

print("--- Iniciando Etapa 4: Enriquecimento de Dados (com Modelo de NLP Aprimorado) ---")

# --- 1. Defini√ß√£o dos Caminhos ---
INPUT_DIR = "data/processed"
OUTPUT_DIR = "data/gold"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# Caminhos dos arquivos de entrada
path_principal = os.path.join(INPUT_DIR, 'dados_principais_limpos.csv')
path_redes = os.path.join(INPUT_DIR, 'dados_redes_limpos.csv')
path_mencoes = os.path.join(INPUT_DIR, 'dados_mencoes_para_nlp.csv')

# --- 2. Carregamento dos Dados Limpos ---
try:
    print("Carregando dados processados...")
    df_principal = pd.read_csv(path_principal)
    df_redes = pd.read_csv(path_redes)
    df_mencoes = pd.read_csv(path_mencoes)
    print("Dados carregados com sucesso.")
except FileNotFoundError as e:
    print(f"\nERRO: Arquivo n√£o encontrado. {e}")
    print("Certifique-se de que a etapa 3 foi executada.")
    exit()

# --- 3. Enriquecimento da Tabela Principal (Feature Engineering) ---
print("\nEnriquecendo a tabela principal com novas features...")
df_principal_enriquecido = df_principal.copy()

# Criando 'faixa_etaria'
bins = [17, 25, 35, 45, 66]
labels = ['Jovem (18-25)', 'Adulto (26-35)', 'Meia-idade (36-45)', 'S√™nior (46+)']
df_principal_enriquecido['faixa_etaria'] = pd.cut(df_principal_enriquecido['idade'], bins=bins, labels=labels, right=True)

# Criando 'perfil_participante'
def criar_perfil(row):
    satisfacao = row['satisfacao_segmento']
    frequencia = row['frequencia_festival']
    if frequencia == '1¬™ vez':
        return 'Novato Satisfeito' if satisfacao >= 4 else 'Novato Insatisfeito'
    else:
        return 'Veterano Satisfeito' if satisfacao >= 4 else 'Veterano Insatisfeito'

df_principal_enriquecido['perfil_participante'] = df_principal_enriquecido.apply(criar_perfil, axis=1)

print("Novas features criadas na tabela principal.")


# --- 4. Enriquecimento da Tabela de Men√ß√µes (An√°lise de Sentimento com pysentimiento) ---
print("\nIniciando an√°lise de sentimento com o modelo 'pysentimiento'...")
df_mencoes_enriquecido = df_mencoes.copy()

# Inicializando o analisador de sentimento para portugu√™s
# Na primeira execu√ß√£o, ele pode fazer o download do modelo (pode levar um minuto)
try:
    analyzer = create_analyzer(task="sentiment", lang="pt")
except Exception as e:
    print(f"Erro ao criar o analisador de sentimento: {e}")
    print("Isso pode ocorrer devido a problemas de rede ou depend√™ncias. Tente executar a c√©lula de instala√ß√£o novamente.")
    exit()

# Fun√ß√£o para obter e classificar o sentimento
def obter_sentimento_pysentimiento(texto):
    resultado = analyzer.predict(texto)
    # O resultado.output √© a classifica√ß√£o: POS, NEU, NEG
    # Vamos mapear para os nossos termos
    mapeamento = {'POS': 'Positivo', 'NEU': 'Neutro', 'NEG': 'Negativo'}
    return mapeamento.get(resultado.output, 'Neutro')

def obter_polaridade_pysentimiento(texto):
    resultado = analyzer.predict(texto)
    # A polaridade √© uma probabilidade para cada classe.
    # Vamos criar uma pontua√ß√£o simples: P(POS) - P(NEG)
    return resultado.probas.get('POS', 0) - resultado.probas.get('NEG', 0)


# Aplicando as fun√ß√µes
print("Calculando sentimento para cada post (pode levar alguns minutos)...")
df_mencoes_enriquecido['sentimento_calculado'] = df_mencoes_enriquecido['conteudo_post'].apply(obter_sentimento_pysentimiento)

print("Calculando pontua√ß√£o de polaridade...")
df_mencoes_enriquecido['polaridade_calculada'] = df_mencoes_enriquecido['conteudo_post'].apply(obter_polaridade_pysentimiento)


print("An√°lise de sentimento aprimorada conclu√≠da.")


# --- 5. Salvando os Dados Enriquecidos (GOLD) ---
print("\nSalvando arquivos finais no diret√≥rio 'gold'...")
# (O c√≥digo de salvamento permanece o mesmo)
path_principal_gold = os.path.join(OUTPUT_DIR, 'dados_principais_gold.csv')
df_principal_enriquecido.to_csv(path_principal_gold, index=False, encoding='utf-8-sig')

path_redes_gold = os.path.join(OUTPUT_DIR, 'dados_redes_gold.csv')
df_redes.to_csv(path_redes_gold, index=False, encoding='utf-8-sig')

path_mencoes_gold = os.path.join(OUTPUT_DIR, 'dados_mencoes_gold.csv')
df_mencoes_enriquecido.to_csv(path_mencoes_gold, index=False, encoding='utf-8-sig')

print(f"Arquivos salvos em '{OUTPUT_DIR}'")


# --- 6. Verifica√ß√£o Final ---
print("\n--- Verifica√ß√£o da Tabela Principal Enriquecida ---")
print(df_principal_enriquecido[['idade', 'faixa_etaria', 'perfil_participante']].head())

print("\n--- Verifica√ß√£o da Tabela de Men√ß√µes Enriquecida (NLP Aprimorado) ---")
print(df_mencoes_enriquecido[['tema_conteudo', 'sentimento_calculado', 'polaridade_calculada', 'conteudo_post']].sample(5, random_state=42))

print("\n--- Compara√ß√£o: Tema Original vs. Sentimento Calculado (Modelo Aprimorado) ---")
print(pd.crosstab(df_mencoes_enriquecido['tema_conteudo'], df_mencoes_enriquecido['sentimento_calculado']))

# Execute este bloco primeiro para instalar a biblioteca para trabalhar com o formato Parquet
!pip install pyarrow
# Instala√ß√£o da biblioteca para manipula√ß√£o de arquivos excel
!pip install openpyxl

# 5_armazenamento.py

import pandas as pd
import os
from google.colab import drive

# --- 1. Montagem do Google Drive ---
# Este passo √© essencial para conectar o Colab ao seu Drive.
# Ele deve ser executado para que o c√≥digo possa encontrar a pasta de destino.
print("Montando o Google Drive...")
try:
    drive.mount('/content/drive', force_remount=True)
    print("Google Drive montado com sucesso!")
except Exception as e:
    print(f"Erro ao montar o Google Drive: {e}")
    # Se a montagem falhar, o script n√£o pode continuar, ent√£o paramos aqui.
    exit()

# --- 2. Defini√ß√£o dos Caminhos ---
print("\n--- Iniciando Etapa 5: Armazenamento Otimizado no Google Drive ---")

# O diret√≥rio de entrada continua sendo a pasta tempor√°ria do Colab
INPUT_DIR = "data/gold"

# ###########################################################################
# # PONTO DE ALTERA√á√ÉO: COLOQUE O CAMINHO DA SUA PASTA AQUI                 #
# ###########################################################################
# Substitua a string abaixo pelo caminho que voc√™ copiou do painel de arquivos.
# Exemplo: '/content/drive/MyDrive/TCC/Meu_Projeto'
GDRIVE_PROJECT_PATH = '/content/drive/MyDrive/MBA BI - USP/TCC/TCC - ENTREGAS/Base de dados'

# O c√≥digo criar√° as subpastas 'data/analytics' dentro do caminho acima.
OUTPUT_DIR = os.path.join(GDRIVE_PROJECT_PATH, "data/analytics")

# Garante que o diret√≥rio de sa√≠da exista no seu Google Drive.
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"Diret√≥rio de sa√≠da configurado para: {OUTPUT_DIR}")


# --- 3. Processamento e Salvamento dos Arquivos ---

# Dicion√°rio com os nomes dos arquivos a serem processados
files_to_process = {
    "dados_principais_gold.csv": "dados_principais",
    "dados_redes_gold.csv": "dados_redes",
    "dados_mencoes_gold.csv": "dados_mencoes"
}

print("\nIniciando a convers√£o dos arquivos 'gold' para Parquet e Excel...")

for csv_file, base_name in files_to_process.items():
    input_path = os.path.join(INPUT_DIR, csv_file)

    try:
        print(f"\nLendo o arquivo: {input_path}")
        df = pd.read_csv(input_path)

        # --- Salvando em formato Parquet no Google Drive ---
        parquet_path = os.path.join(OUTPUT_DIR, f"{base_name}.parquet")
        print(f"--> Salvando em formato otimizado (Parquet): {parquet_path}")
        df.to_parquet(parquet_path, index=False)

        # --- Exportando para formato Excel no Google Drive ---
        excel_path = os.path.join(OUTPUT_DIR, f"{base_name}.xlsx")
        print(f"--> Exportando para consumo humano (Excel): {excel_path}")
        df.to_excel(excel_path, index=False, engine='openpyxl')

    except FileNotFoundError:
        print(f"AVISO: Arquivo de entrada '{input_path}' n√£o encontrado. Pulando este arquivo.")
    except Exception as e:
        print(f"Ocorreu um erro ao processar {csv_file}: {e}")

print("\n--- Pipeline de Dados Conclu√≠do! ---")
print("Os arquivos finais foram salvos no seu Google Drive.")

# --- 4. Verifica√ß√£o Final (Opcional, mas recomendado) ---
print("\n--- Verifica√ß√£o dos Arquivos Finais no Google Drive ---")
print(f"Conte√∫do do diret√≥rio de sa√≠da '{OUTPUT_DIR}':")
try:
    # Lista os arquivos no diret√≥rio de destino para confirmar que foram criados
    final_files = os.listdir(OUTPUT_DIR)
    if not final_files:
        print("Nenhum arquivo encontrado. Verifique se as etapas anteriores foram executadas corretamente.")
    else:
        for f in final_files:
            print(f"- {f}")
except Exception as e:
    print(f"N√£o foi poss√≠vel listar os arquivos: {e}")