# -*- coding: utf-8 -*-
"""Pipeline Geração de Dados Completa

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dy4kZksX78NiHr2-5rcnzHeiGw4H3zV_
"""

from google.colab import drive
drive.mount('/content/drive')

# 1_geracao/base1_participacao.py

import pandas as pd
import numpy as np
import os
import random

print("Iniciando a geração da Base 1: Participantes")

# --- Parâmetros e Pesos para Realismo ---
np.random.seed(42)
random.seed(42)

NUM_PARTICIPANTES = 10000
DIAS_FESTIVAL = ["Pop", "Rock", "Hip-Hop/Rap", "Música Brasileira", "Eletrônico"]

# Distribuição de localização mais realista
LOCALIZACAO_PESOS = {
    "Rio de Janeiro": 0.45, "São Paulo": 0.25, "Belo Horizonte": 0.10,
    "Vitória": 0.05, "Outros": 0.15
}
LOC_DEFEITOS = {
    "Rio de Janeiro": " rj ", "São Paulo": "sao paulo", "Belo Horizonte": "BH"
}

FREQUENCIA_OPCOES = ["Primeira vez", "Ocasional", "Veterano(a)"]

# --- Geração Coluna por Coluna com Causalidade ---

# 1. IDs e Demografia Básica
ids = np.arange(1, NUM_PARTICIPANTES + 1)
idades = np.clip(np.random.normal(loc=28, scale=8, size=NUM_PARTICIPANTES), 18, 65)
generos = np.random.choice(["Masculino", "Feminino", "Outro"], p=[0.48, 0.48, 0.04], size=NUM_PARTICIPANTES)

# 2. Localização (com pesos e defeitos)
localizacoes_limpas = random.choices(
    list(LOCALIZACAO_PESOS.keys()), weights=list(LOCALIZACAO_PESOS.values()), k=NUM_PARTICIPANTES
)
localizacoes_com_defeitos = []
for loc in localizacoes_limpas:
    if random.random() < 0.15: # Introduz defeitos em 15% dos dados
        localizacoes_com_defeitos.append(LOC_DEFEITOS.get(loc, loc))
    else:
        localizacoes_com_defeitos.append(loc)

# 3. Dia do Festival (Causalidade com Idade)
dias_festival = []
for idade in idades:
    if idade < 25:
        dia = np.random.choice(DIAS_FESTIVAL, p=[0.35, 0.15, 0.35, 0.05, 0.1])
    elif idade >= 40:
        dia = np.random.choice(DIAS_FESTIVAL, p=[0.05, 0.40, 0.05, 0.40, 0.1])
    else:
        dia = np.random.choice(DIAS_FESTIVAL)
    dias_festival.append(dia)

# 4. Valor Gasto no Evento (Causalidade com Dia do Festival)
# A lógica simula que o gasto médio varia conforme o dia do festival.
valores_gasto = []
for dia in dias_festival:
    # A base de gasto usa log-normal para realismo (muitos gastam moderadamente, poucos gastam muito)
    base_gasto = np.random.lognormal(mean=5.5, sigma=0.5) # Mediana em torno de R$244

    # Aplica multiplicadores baseados no dia do festival
    if dia in ["Pop", "Rock"]:
        gasto = base_gasto * 1.2
    elif dia == "Hip-Hop/Rap":
        gasto = base_gasto * 1.1
    else:
        gasto = base_gasto * 0.9

    valores_gasto.append(gasto)

valores_gasto = np.array(valores_gasto)
# Introduz valores nulos para simular participantes que não registraram gastos
valores_gasto[np.random.choice(valores_gasto.size, int(NUM_PARTICIPANTES * 0.10), replace=False)] = np.nan

# 5. Frequência (Não Causal)
frequencia_festival = np.random.choice(FREQUENCIA_OPCOES, p=[0.4, 0.35, 0.25], size=NUM_PARTICIPANTES)

# 6. Nota do Lineup (Causal com Dia/Gênero)
notas_lineup = []
for dia in dias_festival:
    if dia in ["Pop", "Rock"]:
        nota = np.random.normal(loc=8.5, scale=1.5)
    elif dia == "Música Brasileira":
        nota = np.random.normal(loc=7.5, scale=1.8)
    else:
        nota = np.random.normal(loc=7.0, scale=2.0)
    notas_lineup.append(round(np.clip(nota, 1, 10), 1))

# 7. Satisfação Geral com o Festival (Causal com Frequência)
satisfacao_festival = []
for freq in frequencia_festival:
    if freq == "Primeira vez":
        satisfacao = np.random.normal(loc=9.0, scale=1.0)
    elif freq == "Ocasional":
        satisfacao = np.random.normal(loc=8.0, scale=1.5)
    else:
        satisfacao = np.random.normal(loc=7.5, scale=2.0)
    satisfacao_festival.append(round(np.clip(satisfacao, 1, 10), 1))

# --- Montagem e Finalização do DataFrame ---
df_participantes = pd.DataFrame({
    'id_participante': ids,
    'idade': idades.astype(int),
    'genero': generos,
    'localizacao': localizacoes_com_defeitos,
    'valor_gasto': np.round(valores_gasto, 2),
    'frequencia_festival': frequencia_festival,
    'dia_festival': dias_festival,
    'nota_lineup': notas_lineup,
    'satisfacao_festival': satisfacao_festival
})

# Introduzindo defeito final: tipo de dado inconsistente em 'idade'
indices_defeito = df_participantes.sample(frac=0.1, random_state=42).index
df_participantes.loc[indices_defeito, 'idade'] = df_participantes.loc[indices_defeito, 'idade'].astype(str)

# --- Salvando o arquivo ---
output_dir = "data/raw"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, 'base1_participantes.csv')
df_participantes.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"Base 1 gerada com sucesso! {len(df_participantes)} linhas.")
print(f"Arquivo salvo em: {output_path}")
print("\nAmostra dos dados gerados:")
print(df_participantes.head())

# 1_geracao/base2_marcas.py

import pandas as pd
import numpy as np
import os
import random

print("Iniciando a geração da Base 2 (versão com Segmentos): Interações de Mercado")

# --- Configurações ---
np.random.seed(42)
random.seed(42)

# --- Carregar a Base 1 para usar seus dados na geração ---
path_b1 = "data/raw/base1_participantes.csv"
if not os.path.exists(path_b1):
    print(f"Erro: Arquivo '{path_b1}' não encontrado. Execute o script base1_participacao.py primeiro.")
    exit()

df_participantes = pd.read_csv(path_b1)

# --- Parâmetros dos Segmentos e Interações (SEM NOMES DE MARCAS) ---
SEGMENTOS_DISPONIVEIS = [
    "Serviços Financeiros", "Bebida Alcoólica", "Postos de Combustível",
    "Alimentos", "Seguros", "Telecomunicações", "Moda", "Automobilismo",
    "Bebida Não Alcoólica", "Beleza", "Chocolates"
]
TIPO_INTERACAO_PESOS = {
    "Compra de Produto": 0.5,
    "Visita a Estande": 0.3,
    "Participou de Ativação": 0.2
}

# --- Geração das Interações ---
interacoes_lista = []
id_interacao_counter = 1

print("Gerando interações com segmentos para cada participante...")
for _, participante in df_participantes.iterrows():
    # Cada participante interage com 1 a 4 segmentos
    num_interacoes = random.randint(1, 4)
    segmentos_interagidos = random.sample(SEGMENTOS_DISPONIVEIS, k=num_interacoes)

    for segmento in segmentos_interagidos:
        # Gerando as variáveis independentes primeiro
        tempo_de_espera = round(np.random.lognormal(mean=2.5, sigma=0.8)) # Em minutos
        avaliacao_experiencia = random.randint(1, 5)

        if random.random() < 0.4: # 40% de chance de ter brinde
            avaliacao_brinde = random.randint(1, 5)
        else:
            avaliacao_brinde = np.nan # Defeito: NaN para interações sem brinde

        # Relação Causal #1 (Forte): satisfacao_marca depende de experiencia, tempo e brinde
        tempo_espera_normalizado = min(tempo_de_espera / 30, 1) # Penalidade máxima após 30 min
        nota_base = (avaliacao_experiencia * 0.7) - (tempo_espera_normalizado * 0.5)
        if not np.isnan(avaliacao_brinde):
             nota_base += (avaliacao_brinde * 0.2)
        satisfacao_segmento = round(np.clip(nota_base + np.random.normal(0, 0.3), 1, 5), 1)

        # Relação Causal #5 (Esperada): recomendaria_segmento depende da satisfação
        prob_recomendar = (satisfacao_segmento - 1) / 4
        recomendaria_segmento = random.random() < prob_recomendar

        interacoes_lista.append({
            'id_interacao': id_interacao_counter,
            'id_participante': participante['id_participante'],
            'segmento_interagido': segmento,
            'tipo_interacao': random.choices(list(TIPO_INTERACAO_PESOS.keys()), weights=list(TIPO_INTERACAO_PESOS.values()), k=1)[0],
            'tempo_de_espera': tempo_de_espera,
            'avaliacao_experiencia_ativacao': avaliacao_experiencia,
            'avaliacao_brinde': avaliacao_brinde,
            'satisfacao_segmento': satisfacao_segmento,
            'recomendaria_segmento': recomendaria_segmento
        })
        id_interacao_counter += 1

df_interacoes = pd.DataFrame(interacoes_lista)

# --- Introdução de Defeitos Adicionais para Pré-processamento ---
# 1. Defeito no tempo_de_espera: Alguns valores como string "X min"
indices_defeito_tempo = df_interacoes.sample(frac=0.1, random_state=1).index
df_interacoes.loc[indices_defeito_tempo, 'tempo_de_espera'] = df_interacoes.loc[indices_defeito_tempo, 'tempo_de_espera'].astype(str) + ' min'

# 2. Defeito em satisfacao_segmento: Outliers (valores fora da escala 1-5)
indices_defeito_satisfacao = df_interacoes.sample(frac=0.03, random_state=2).index
df_interacoes.loc[indices_defeito_satisfacao, 'satisfacao_segmento'] = random.choices([0, -1, 6, 7.5], k=len(indices_defeito_satisfacao))

# --- Passo de Enriquecimento: Atualizar a Base 1 ---
# Relação Causal #7: O segmento mais lembrado é aquele com maior satisfação
print("Enriquecendo a Base 1 com o segmento mais lembrado...")
idx_max_satisfacao = df_interacoes.groupby('id_participante')['satisfacao_segmento'].idxmax()
df_melhor_experiencia = df_interacoes.loc[idx_max_satisfacao][['id_participante', 'segmento_interagido']]

mapeamento_segmento = df_melhor_experiencia.set_index('id_participante')['segmento_interagido']
df_participantes['segmento_mais_lembrado'] = df_participantes['id_participante'].map(mapeamento_segmento)

# --- Salvando os Arquivos ---
output_dir = "data/raw"
os.makedirs(output_dir, exist_ok=True)

# Salva a Base 2
path_b2 = os.path.join(output_dir, 'base2_marcas.csv')
df_interacoes.to_csv(path_b2, index=False, encoding='utf-8-sig')
print(f"\nBase 2 gerada com sucesso! {len(df_interacoes)} linhas.")
print(f"Arquivo salvo em: {path_b2}")

# Salva novamente a Base 1, agora enriquecida
df_participantes.to_csv(path_b1, index=False, encoding='utf-8-sig')
print(f"\nBase 1 foi atualizada/enriquecida com a coluna 'segmento_mais_lembrado'.")
print(f"Arquivo salvo em: {path_b1}")

print("\n🔍 Verificação da Base 2:")
print("---------------------------------------")
print("Amostra da base de interações com segmentos:")
print(df_interacoes.sample(5, random_state=42))
print("\nVerificação do enriquecimento na Base 1:")
print(df_participantes[['id_participante', 'segmento_mais_lembrado']].dropna().head())

# 1_geracao/base3_redes.py

import pandas as pd
import numpy as np
import os
import random
from datetime import datetime, timedelta

print("Iniciando a geração da Base 3 (versão aprimorada com Alcance, Impressões e Salvamentos)")

# --- Configurações ---
np.random.seed(42)
random.seed(42)

DIAS_FESTIVAL = ["Pop", "Rock", "Hip-Hop/Rap", "Música Brasileira", "Eletrônico"]
REDES_SOCIAIS = ["Instagram", "Twitter", "TikTok"]
DATA_INICIO_FESTIVAL = datetime(2024, 9, 13)
POSTS_POR_DIA_POR_REDE = 10

# --- Geração dos Posts ---
posts_lista = []
id_post_counter = 1

print("Gerando posts da campanha da marca para cada dia do festival...")
for i, dia in enumerate(DIAS_FESTIVAL):
    data_do_dia = DATA_INICIO_FESTIVAL + timedelta(days=i)

    for rede in REDES_SOCIAIS:
        for _ in range(POSTS_POR_DIA_POR_REDE):

            # --- Lógica de Geração Revisada (Funil de Marketing Digital) ---

            # 1. Estratégia de Conteúdo (como antes)
            if rede == "TikTok": tipo_conteudo = "Vídeo Curto"
            elif rede == "Instagram": tipo_conteudo = random.choice(["Reels", "Vídeo Curto"]) if dia in ["Pop", "Eletrônico"] else random.choice(["Foto", "Carrossel"])
            else: tipo_conteudo = "Texto com Imagem"

            # 2. Geração de Alcance (Reach) - Pessoas Únicas
            # Simula que TikTok tem maior alcance orgânico, seguido por Instagram.
            if rede == "TikTok": base_alcance = np.random.randint(80000, 300000)
            elif rede == "Instagram": base_alcance = np.random.randint(50000, 200000)
            else: base_alcance = np.random.randint(10000, 50000)

            # Formatos de vídeo têm maior potencial de alcance
            if tipo_conteudo in ["Reels", "Vídeo Curto"]: base_alcance *= 1.4

            # Multiplicador de "hype" do festival
            multiplicador_dia = 1.3 - (i * 0.1)
            alcance = int(base_alcance * multiplicador_dia)

            # 3. Geração de Impressões (Impressions) a partir do Alcance
            # Impressões = Alcance * Frequência (quantas vezes em média cada pessoa viu)
            frequencia = np.random.uniform(1.1, 1.8) # Cada pessoa viu o post entre 1.1 e 1.8 vezes em média
            impressoes = int(alcance * frequencia)

            # 4. Geração de Curtidas a partir das Impressões
            # Taxa de engajamento (curtidas/impressões) varia
            taxa_de_curtidas = np.random.uniform(0.03, 0.12) # Entre 3% e 12%
            curtidas = int(impressoes * taxa_de_curtidas)

            # 5. Geração de outras interações a partir das Curtidas
            comentarios = int(curtidas * np.random.uniform(0.01, 0.04))
            compartilhamentos = int(curtidas * np.random.uniform(0.02, 0.05))
            # Salvamentos são uma ação de alta intenção, geralmente menos frequentes
            salvamentos = int(curtidas * np.random.uniform(0.005, 0.03))


            # --- Introdução de Defeitos (como antes) ---
            if random.random() < 0.3: data_postagem = data_do_dia.strftime('%d/%m/%Y')
            else: data_postagem = data_do_dia.strftime('%Y-%m-%d')

            posts_lista.append({
                'id_post': id_post_counter,
                'data_postagem': data_postagem,
                'rede_social': rede,
                'dia_festival': dia,
                'tipo_conteudo': tipo_conteudo,
                'alcance': alcance,
                'impressoes': impressoes,
                'curtidas': curtidas,
                'comentarios': comentarios,
                'compartilhamentos': compartilhamentos,
                'salvamentos': salvamentos
            })
            id_post_counter += 1

df_redes = pd.DataFrame(posts_lista)

# Defeito no formato das métricas (formato "k")
indices_defeito_k = df_redes[df_redes['curtidas'] > 1000].sample(frac=0.2, random_state=1).index
for idx in indices_defeito_k:
    valor = df_redes.loc[idx, 'curtidas']
    df_redes.loc[idx, 'curtidas'] = f"{valor/1000:.1f}k"
    valor_comentario = df_redes.loc[idx, 'comentarios']
    if valor_comentario > 1000:
       df_redes.loc[idx, 'comentarios'] = f"{valor_comentario/1000:.1f}k"

# --- Salvando o arquivo ---
output_dir = "data/raw"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, 'base3_redes.csv')
df_redes.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"\nBase 3 (versão final) gerada com sucesso! {len(df_redes)} linhas.")
print(f"Arquivo salvo em: {output_path}")

print("\nAmostra dos dados gerados com as novas métricas:")
display(df_redes.head())

# 1_geracao/base4_marca_aberta.py

import pandas as pd
import numpy as np
import os
import random

try:
    from faker import Faker
    fake = Faker('pt_BR')
except ImportError:
    print("Biblioteca Faker não encontrada. Usando fallback para nomes de usuário.")
    fake = None

print("Iniciando a geração da Base 4 (versão 'deep dive'): Menções Abertas dos Usuários")

# --- Configurações ---
np.random.seed(42)
random.seed(42)

# --- Carregar as bases anteriores ---
path_b1 = "data/raw/base1_participantes.csv"
path_b2 = "data/raw/base2_marcas.csv"

if not os.path.exists(path_b1) or not os.path.exists(path_b2):
    print("Erro: Arquivos das bases 1 ou 2 não encontrados. Execute os scripts anteriores primeiro.")
    exit()

df_participantes = pd.read_csv(path_b1)
df_interacoes = pd.read_csv(path_b2, low_memory=False)

# --- Limpeza preliminar da Base 2 ---
def limpar_tempo(valor):
    if isinstance(valor, str):
        return int(valor.replace(' min', ''))
    return int(valor)
df_interacoes['tempo_de_espera_limpo'] = df_interacoes['tempo_de_espera'].apply(limpar_tempo)

# --- Lógica de "Deep Dive" em um Segmento Específico ---
SEGMENTO_ALVO = "Bebida Alcoólica"
print(f"Filtrando interações para gerar menções apenas para o segmento: '{SEGMENTO_ALVO}'")

# Para adicionar o 'dia_festival', precisamos primeiro juntar a Base 1 e 2
# Apenas as colunas necessárias para evitar sobrecarga de memória
df_interacoes_enriquecido = pd.merge(
    df_interacoes,
    df_participantes[['id_participante', 'dia_festival']],
    on='id_participante',
    how='left'
)

# Agora, filtramos o dataframe já enriquecido
df_interacoes_filtrado = df_interacoes_enriquecido[df_interacoes_enriquecido['segmento_interagido'] == SEGMENTO_ALVO].copy()

if df_interacoes_filtrado.empty:
    print(f"Nenhuma interação encontrada para o segmento '{SEGMENTO_ALVO}'. A Base 4 não será gerada.")
    exit()

# --- Parâmetros ---
fracao_amostra = 0.5
NUM_MENCOES = int(len(df_interacoes_filtrado) * fracao_amostra)
REDES_SOCIAIS = ['Twitter', 'Instagram']
TEMPLATES = {
    'elogio': [
        "A experiência no estande de {segmento} foi SENSACIONAL! Valeu cada segundo. #Festival",
        "Que organização incrível no espaço de {segmento}. Atendimento nota 10/10. ✨",
        "Simplesmente amei o brinde que ganhei de {segmento}! Já quero o próximo festival."
    ],
    'reclamacao': [
        "Absurdo o tempo de espera na fila de {segmento}: {tempo_espera} minutos pra nada. 😡 #Fail",
        "Totalmente decepcionado com a ativação de {segmento}. Esperava muito mais.",
        "O pessoal de {segmento} parecia perdido, atendimento péssimo."
    ],
    'neutro': [
        "Passei pelo estande de {segmento} hoje no festival.",
        "Vi que {segmento} é um dos patrocinadores este ano.",
        "Alguém sabe o que tá rolando na ativação de {segmento}?"
    ]
}

# --- Geração dos Posts ---
mencoes_lista = []
id_mencao_counter = 1
print(f"Gerando {NUM_MENCOES} menções de usuários com base em suas interações com '{SEGMENTO_ALVO}'...")

amostra_interacoes = df_interacoes_filtrado.sample(n=NUM_MENCOES, random_state=42)

for _, interacao in amostra_interacoes.iterrows():
    satisfacao = interacao['satisfacao_segmento']
    tempo_espera = interacao['tempo_de_espera_limpo']
    dia_do_festival = interacao['dia_festival'] # Capturando o dia do festival

    # Definir o tom do post com base na satisfação
    if satisfacao >= 4.0: tema_conteudo = 'elogio'
    elif satisfacao <= 2.5: tema_conteudo = 'reclamacao'
    else: tema_conteudo = 'neutro'

    conteudo = random.choice(TEMPLATES[tema_conteudo]).format(
        segmento=SEGMENTO_ALVO, tempo_espera=tempo_espera
    )

    # Adicionar ruído linguístico
    if random.random() < 0.3: conteudo = conteudo.replace("que", "q").replace("nao", "n")
    if random.random() < 0.6: conteudo += " " + random.choice(['kkkkk', '😂', '🤡', 'top', 'aff', '🤔'])

    # Gerar nome de usuário
    if fake: nome_usuario = fake.first_name().lower() + str(random.randint(10, 99))
    else: nome_usuario = f"user{interacao['id_participante']}"

    mencoes_lista.append({
        'id_mencao': id_mencao_counter,
        'id_participante': interacao['id_participante'],
        'nome_usuario': nome_usuario,
        'rede_social': random.choice(REDES_SOCIAIS),
        'segmento_mencionado': SEGMENTO_ALVO,      # <-- COLUNA PREENCHIDA DIRETAMENTE
        'dia_festival': dia_do_festival,           # <-- NOVA COLUNA ADICIONADA
        'tema_conteudo': tema_conteudo,
        'conteudo_post': conteudo
    })
    id_mencao_counter += 1

df_mencoes = pd.DataFrame(mencoes_lista)

# --- Salvando o arquivo ---
output_dir = "data/raw"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, 'base4_marca_aberta.csv')
df_mencoes.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"\nBase 4 gerada com sucesso! {len(df_mencoes)} linhas.")
print(f"Arquivo salvo em: {output_path}")

print("\nAmostra dos dados gerados:")
display(df_mencoes.head())

# 2_unificacao.py

import pandas as pd
import os

print("--- Iniciando Etapa 2: Unificação dos Dados ---")

# --- 1. Definição dos Caminhos ---
INPUT_DIR = "data/raw"
OUTPUT_DIR = "data/interim" # Usaremos um diretório intermediário para os dados processados

# Criar o diretório de saída se ele não existir
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Caminhos dos arquivos de entrada
path_b1 = os.path.join(INPUT_DIR, 'base1_participantes.csv')
path_b2 = os.path.join(INPUT_DIR, 'base2_marcas.csv')
path_b3 = os.path.join(INPUT_DIR, 'base3_redes.csv')
path_b4 = os.path.join(INPUT_DIR, 'base4_marca_aberta.csv')

# Caminho do arquivo de saída
output_path = os.path.join(OUTPUT_DIR, 'dados_unificados_para_processamento.csv')


# --- 2. Carregamento dos Dados ---
try:
    print(f"Carregando dados de '{path_b1}'...")
    df_participantes = pd.read_csv(path_b1)

    print(f"Carregando dados de '{path_b2}'...")
    df_interacoes = pd.read_csv(path_b2, low_memory=False)

    print(f"Carregando dados de '{path_b3}'...")
    df_redes_sociais = pd.read_csv(path_b3)

    print(f"Carregando dados de '{path_b4}'...")
    df_mencoes = pd.read_csv(path_b4)

    print("\nTodos os arquivos foram carregados com sucesso.")

except FileNotFoundError as e:
    print(f"\nERRO: Arquivo não encontrado. {e}")
    print("Por favor, certifique-se de que todos os scripts da etapa '1_geracao' foram executados.")
    exit()

# --- 3. Unificação Principal (Junção da Base 1 e Base 2) ---
# O "grão" da nossa análise principal será a INTERAÇÃO.
# Portanto, vamos enriquecer a tabela de interações com os dados dos participantes.
# Usamos um left join para garantir que todas as interações sejam mantidas.

print("\nUnificando a base de interações com a base de participantes...")
df_principal = pd.merge(
    df_interacoes,
    df_participantes,
    on='id_participante',
    how='left'
)

print(f"A tabela principal unificada tem {len(df_principal)} linhas e {len(df_principal.columns)} colunas.")

# --- 4. Armazenamento ---
# Neste ponto, salvamos a tabela principal unificada.
# As outras tabelas (df_redes_sociais e df_mencoes) serão usadas na próxima etapa,
# mas não precisam ser juntadas agora para evitar duplicação massiva de dados.

print(f"\nSalvando a tabela principal unificada em '{output_path}'...")
df_principal.to_csv(output_path, index=False, encoding='utf-8-sig')

print("\n--- Etapa 2: Unificação concluída com sucesso! ---")
print(f"O arquivo principal para a próxima etapa está pronto.")
print("\nAmostra da tabela unificada:")
print(df_principal.head())

# 3_preprocessamento.py

import pandas as pd
import numpy as np
import os

print("--- Iniciando Etapa 3: Pré-processamento e Limpeza dos Dados ---")

# --- 1. Definição dos Caminhos ---
INPUT_DIR = "data/interim"
RAW_DIR = "data/raw"
OUTPUT_DIR = "data/processed"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# Caminhos dos arquivos de entrada
path_principal = os.path.join(INPUT_DIR, 'dados_unificados_para_processamento.csv')
path_redes = os.path.join(RAW_DIR, 'base3_redes.csv')
path_mencoes = os.path.join(RAW_DIR, 'base4_marca_aberta.csv')

# --- 2. Carregamento dos Dados ---
try:
    print("Carregando dados unificados...")
    df_principal = pd.read_csv(path_principal, low_memory=False)

    print("Carregando dados de redes sociais...")
    df_redes = pd.read_csv(path_redes)

    print("Carregando dados de menções...")
    df_mencoes = pd.read_csv(path_mencoes)

    print("\nArquivos carregados com sucesso.")
except FileNotFoundError as e:
    print(f"\nERRO: Arquivo não encontrado. {e}")
    print("Certifique-se de que as etapas 1 e 2 foram executadas.")
    exit()


# --- 3. Funções de Limpeza (Helpers) ---
def padronizar_localizacao(loc):
    """Padroniza a coluna de localização, corrigindo abreviações e capitalização."""
    if pd.isna(loc): return None
    loc = str(loc).strip().lower()
    mapeamento = {'rj': 'Rio de Janeiro', 'sao paulo': 'São Paulo', 'bh': 'Belo Horizonte'}
    return mapeamento.get(loc, loc.title())

def limpar_metricas_k(valor):
    """Converte métricas de texto com 'k' (e.g., '15.2k') em valores numéricos."""
    if isinstance(valor, str):
        valor = valor.lower()
        if 'k' in valor:
            return float(valor.replace('k', '')) * 1000
    return pd.to_numeric(valor, errors='coerce')

# --- 4. Processamento da Tabela Principal ---
print("\nIniciando limpeza da tabela principal...")
df_principal_limpo = df_principal.copy()

# Tratamento da coluna 'idade'
# Converte para numérico, tratando erros que foram inseridos propositalmente.
if 'idade' in df_principal_limpo.columns:
    df_principal_limpo['idade'] = pd.to_numeric(df_principal_limpo['idade'], errors='coerce')
    # Imputa a mediana para quaisquer valores que não puderam ser convertidos
    mediana_idade = df_principal_limpo['idade'].median()
    df_principal_limpo['idade'].fillna(mediana_idade, inplace=True)
    df_principal_limpo['idade'] = df_principal_limpo['idade'].astype(int)

# Tratamento da coluna 'localizacao'
# Aplica a função de padronização para unificar os nomes das cidades.
if 'localizacao' in df_principal_limpo.columns:
    df_principal_limpo['localizacao'] = df_principal_limpo['localizacao'].apply(padronizar_localizacao)

# Tratamento da coluna 'valor_gasto' (antiga renda_mensal)
# Imputa a mediana para preencher valores ausentes (NaN).
if 'valor_gasto' in df_principal_limpo.columns:
    mediana_gasto = df_principal_limpo['valor_gasto'].median()
    df_principal_limpo['valor_gasto'].fillna(mediana_gasto, inplace=True)
else:
    print("Aviso: Coluna 'valor_gasto' não encontrada na tabela principal.")

# Tratamento da coluna 'tempo_de_espera'
# Remove o sufixo "min" e converte a coluna para o tipo numérico.
if 'tempo_de_espera' in df_principal_limpo.columns:
    df_principal_limpo['tempo_de_espera'] = df_principal_limpo['tempo_de_espera'].astype(str).str.replace('min', '', regex=False).str.strip()
    df_principal_limpo['tempo_de_espera'] = pd.to_numeric(df_principal_limpo['tempo_de_espera'], errors='coerce')

# Tratamento da coluna 'avaliacao_brinde'
# Imputa 0 nos valores nulos, representando explicitamente a ausência de brinde.
if 'avaliacao_brinde' in df_principal_limpo.columns:
    df_principal_limpo['avaliacao_brinde'].fillna(0, inplace=True)

# Tratamento da coluna 'satisfacao_segmento'
# Aplica a técnica de "clipping" para corrigir outliers e garantir que os valores fiquem na escala de 1 a 5.
if 'satisfacao_segmento' in df_principal_limpo.columns:
    df_principal_limpo['satisfacao_segmento'] = df_principal_limpo['satisfacao_segmento'].clip(lower=1, upper=5)

print("Limpeza da tabela principal concluída.")

# --- 5. Processamento da Tabela de Redes Sociais ---
print("\nIniciando limpeza da tabela de redes sociais...")
df_redes_limpo = df_redes.copy()

# Tratamento da coluna 'data_postagem'
# Converte os formatos mistos (ISO e brasileiro) para um tipo datetime padronizado.
df_redes_limpo['data_postagem'] = pd.to_datetime(df_redes_limpo['data_postagem'], errors='coerce', dayfirst=False, format=None)

# Tratamento das colunas de métricas
# Aplica a função de limpeza para converter valores como "37.8k" em 37800.
df_redes_limpo['curtidas'] = df_redes_limpo['curtidas'].apply(limpar_metricas_k)
df_redes_limpo['comentarios'] = df_redes_limpo['comentarios'].apply(limpar_metricas_k)
df_redes_limpo['compartilhamentos'] = pd.to_numeric(df_redes_limpo['compartilhamentos'], errors='coerce')

print("Limpeza da tabela de redes sociais concluída.")

# --- 6. Salvando os Dados Processados ---
print("\nSalvando arquivos processados...")

path_principal_limpo = os.path.join(OUTPUT_DIR, 'dados_principais_limpos.csv')
df_principal_limpo.to_csv(path_principal_limpo, index=False, encoding='utf-8-sig')

path_redes_limpo = os.path.join(OUTPUT_DIR, 'dados_redes_limpos.csv')
df_redes_limpo.to_csv(path_redes_limpo, index=False, encoding='utf-8-sig')

# A base de menções é salva sem grandes alterações estruturais, pronta para o NLP.
path_mencoes_processado = os.path.join(OUTPUT_DIR, 'dados_mencoes_para_nlp.csv')
df_mencoes.to_csv(path_mencoes_processado, index=False, encoding='utf-8-sig')

print(f"Arquivos salvos em '{OUTPUT_DIR}'")

# --- 7. Verificação Final ---
print("\n--- Verificação Final da Tabela Principal Limpa ---")
df_principal_limpo.info()

print("\nAmostra da tabela principal após a limpeza:")
print(df_principal_limpo.head())

!pip install pysentimiento

# 4_enriquecimento.py

import pandas as pd
import numpy as np
import os
import re
# Importando a nova biblioteca para análise de sentimento
from pysentimiento import create_analyzer

print("--- Iniciando Etapa 4: Enriquecimento de Dados (com Modelo de NLP Aprimorado) ---")

# --- 1. Definição dos Caminhos ---
INPUT_DIR = "data/processed"
OUTPUT_DIR = "data/gold"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# Caminhos dos arquivos de entrada
path_principal = os.path.join(INPUT_DIR, 'dados_principais_limpos.csv')
path_redes = os.path.join(INPUT_DIR, 'dados_redes_limpos.csv')
path_mencoes = os.path.join(INPUT_DIR, 'dados_mencoes_para_nlp.csv')

# --- 2. Carregamento dos Dados Limpos ---
try:
    print("Carregando dados processados...")
    df_principal = pd.read_csv(path_principal)
    df_redes = pd.read_csv(path_redes)
    df_mencoes = pd.read_csv(path_mencoes)
    print("Dados carregados com sucesso.")
except FileNotFoundError as e:
    print(f"\nERRO: Arquivo não encontrado. {e}")
    print("Certifique-se de que a etapa 3 foi executada.")
    exit()

# --- 3. Enriquecimento da Tabela Principal (Feature Engineering) ---
print("\nEnriquecendo a tabela principal com novas features...")
df_principal_enriquecido = df_principal.copy()

# Criando 'faixa_etaria'
bins = [17, 25, 35, 45, 66]
labels = ['Jovem (18-25)', 'Adulto (26-35)', 'Meia-idade (36-45)', 'Sênior (46+)']
df_principal_enriquecido['faixa_etaria'] = pd.cut(df_principal_enriquecido['idade'], bins=bins, labels=labels, right=True)

# Criando 'perfil_participante'
def criar_perfil(row):
    satisfacao = row['satisfacao_segmento']
    frequencia = row['frequencia_festival']
    if frequencia == '1ª vez':
        return 'Novato Satisfeito' if satisfacao >= 4 else 'Novato Insatisfeito'
    else:
        return 'Veterano Satisfeito' if satisfacao >= 4 else 'Veterano Insatisfeito'

df_principal_enriquecido['perfil_participante'] = df_principal_enriquecido.apply(criar_perfil, axis=1)

print("Novas features criadas na tabela principal.")


# --- 4. Enriquecimento da Tabela de Menções (Análise de Sentimento com pysentimiento) ---
print("\nIniciando análise de sentimento com o modelo 'pysentimiento'...")
df_mencoes_enriquecido = df_mencoes.copy()

# Inicializando o analisador de sentimento para português
# Na primeira execução, ele pode fazer o download do modelo (pode levar um minuto)
try:
    analyzer = create_analyzer(task="sentiment", lang="pt")
except Exception as e:
    print(f"Erro ao criar o analisador de sentimento: {e}")
    print("Isso pode ocorrer devido a problemas de rede ou dependências. Tente executar a célula de instalação novamente.")
    exit()

# Função para obter e classificar o sentimento
def obter_sentimento_pysentimiento(texto):
    resultado = analyzer.predict(texto)
    # O resultado.output é a classificação: POS, NEU, NEG
    # Vamos mapear para os nossos termos
    mapeamento = {'POS': 'Positivo', 'NEU': 'Neutro', 'NEG': 'Negativo'}
    return mapeamento.get(resultado.output, 'Neutro')

def obter_polaridade_pysentimiento(texto):
    resultado = analyzer.predict(texto)
    # A polaridade é uma probabilidade para cada classe.
    # Vamos criar uma pontuação simples: P(POS) - P(NEG)
    return resultado.probas.get('POS', 0) - resultado.probas.get('NEG', 0)


# Aplicando as funções
print("Calculando sentimento para cada post (pode levar alguns minutos)...")
df_mencoes_enriquecido['sentimento_calculado'] = df_mencoes_enriquecido['conteudo_post'].apply(obter_sentimento_pysentimiento)

print("Calculando pontuação de polaridade...")
df_mencoes_enriquecido['polaridade_calculada'] = df_mencoes_enriquecido['conteudo_post'].apply(obter_polaridade_pysentimiento)


print("Análise de sentimento aprimorada concluída.")


# --- 5. Salvando os Dados Enriquecidos (GOLD) ---
print("\nSalvando arquivos finais no diretório 'gold'...")
# (O código de salvamento permanece o mesmo)
path_principal_gold = os.path.join(OUTPUT_DIR, 'dados_principais_gold.csv')
df_principal_enriquecido.to_csv(path_principal_gold, index=False, encoding='utf-8-sig')

path_redes_gold = os.path.join(OUTPUT_DIR, 'dados_redes_gold.csv')
df_redes.to_csv(path_redes_gold, index=False, encoding='utf-8-sig')

path_mencoes_gold = os.path.join(OUTPUT_DIR, 'dados_mencoes_gold.csv')
df_mencoes_enriquecido.to_csv(path_mencoes_gold, index=False, encoding='utf-8-sig')

print(f"Arquivos salvos em '{OUTPUT_DIR}'")


# --- 6. Verificação Final ---
print("\n--- Verificação da Tabela Principal Enriquecida ---")
print(df_principal_enriquecido[['idade', 'faixa_etaria', 'perfil_participante']].head())

print("\n--- Verificação da Tabela de Menções Enriquecida (NLP Aprimorado) ---")
print(df_mencoes_enriquecido[['tema_conteudo', 'sentimento_calculado', 'polaridade_calculada', 'conteudo_post']].sample(5, random_state=42))

print("\n--- Comparação: Tema Original vs. Sentimento Calculado (Modelo Aprimorado) ---")
print(pd.crosstab(df_mencoes_enriquecido['tema_conteudo'], df_mencoes_enriquecido['sentimento_calculado']))

# Execute este bloco primeiro para instalar a biblioteca para trabalhar com o formato Parquet
!pip install pyarrow
# Instalação da biblioteca para manipulação de arquivos excel
!pip install openpyxl

# 5_armazenamento.py

import pandas as pd
import os
from google.colab import drive

# --- 1. Montagem do Google Drive ---
# Este passo é essencial para conectar o Colab ao seu Drive.
# Ele deve ser executado para que o código possa encontrar a pasta de destino.
print("Montando o Google Drive...")
try:
    drive.mount('/content/drive', force_remount=True)
    print("Google Drive montado com sucesso!")
except Exception as e:
    print(f"Erro ao montar o Google Drive: {e}")
    # Se a montagem falhar, o script não pode continuar, então paramos aqui.
    exit()

# --- 2. Definição dos Caminhos ---
print("\n--- Iniciando Etapa 5: Armazenamento Otimizado no Google Drive ---")

# O diretório de entrada continua sendo a pasta temporária do Colab
INPUT_DIR = "data/gold"

# ###########################################################################
# # PONTO DE ALTERAÇÃO: COLOQUE O CAMINHO DA SUA PASTA AQUI                 #
# ###########################################################################
# Substitua a string abaixo pelo caminho que você copiou do painel de arquivos.
# Exemplo: '/content/drive/MyDrive/TCC/Meu_Projeto'
GDRIVE_PROJECT_PATH = '/content/drive/MyDrive/MBA BI - USP/TCC/TCC - ENTREGAS/Base de dados'

# O código criará as subpastas 'data/analytics' dentro do caminho acima.
OUTPUT_DIR = os.path.join(GDRIVE_PROJECT_PATH, "data/analytics")

# Garante que o diretório de saída exista no seu Google Drive.
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"Diretório de saída configurado para: {OUTPUT_DIR}")


# --- 3. Processamento e Salvamento dos Arquivos ---

# Dicionário com os nomes dos arquivos a serem processados
files_to_process = {
    "dados_principais_gold.csv": "dados_principais",
    "dados_redes_gold.csv": "dados_redes",
    "dados_mencoes_gold.csv": "dados_mencoes"
}

print("\nIniciando a conversão dos arquivos 'gold' para Parquet e Excel...")

for csv_file, base_name in files_to_process.items():
    input_path = os.path.join(INPUT_DIR, csv_file)

    try:
        print(f"\nLendo o arquivo: {input_path}")
        df = pd.read_csv(input_path)

        # --- Salvando em formato Parquet no Google Drive ---
        parquet_path = os.path.join(OUTPUT_DIR, f"{base_name}.parquet")
        print(f"--> Salvando em formato otimizado (Parquet): {parquet_path}")
        df.to_parquet(parquet_path, index=False)

        # --- Exportando para formato Excel no Google Drive ---
        excel_path = os.path.join(OUTPUT_DIR, f"{base_name}.xlsx")
        print(f"--> Exportando para consumo humano (Excel): {excel_path}")
        df.to_excel(excel_path, index=False, engine='openpyxl')

    except FileNotFoundError:
        print(f"AVISO: Arquivo de entrada '{input_path}' não encontrado. Pulando este arquivo.")
    except Exception as e:
        print(f"Ocorreu um erro ao processar {csv_file}: {e}")

print("\n--- Pipeline de Dados Concluído! ---")
print("Os arquivos finais foram salvos no seu Google Drive.")

# --- 4. Verificação Final (Opcional, mas recomendado) ---
print("\n--- Verificação dos Arquivos Finais no Google Drive ---")
print(f"Conteúdo do diretório de saída '{OUTPUT_DIR}':")
try:
    # Lista os arquivos no diretório de destino para confirmar que foram criados
    final_files = os.listdir(OUTPUT_DIR)
    if not final_files:
        print("Nenhum arquivo encontrado. Verifique se as etapas anteriores foram executadas corretamente.")
    else:
        for f in final_files:
            print(f"- {f}")
except Exception as e:
    print(f"Não foi possível listar os arquivos: {e}")